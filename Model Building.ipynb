{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 6\n",
    "img_rows, img_cols = 48, 48\n",
    "batch_size = 16\n",
    "\n",
    "train_data_dir = 'fer2013/train'\n",
    "validation_data_dir = 'fer2013/validation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Train Data and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28273 images belonging to 6 classes.\n",
      "Found 3534 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "img_train_gen=ImageDataGenerator(rescale=1.0/255,\\\n",
    "                                  rotation_range=30,\\\n",
    "                                  shear_range=0.3,\\\n",
    "                                  zoom_range=0.3,\\\n",
    "                                  width_shift_range=0.4,\\\n",
    "                                  height_shift_range=0.4,\\\n",
    "                                  horizontal_flip=True,\\\n",
    "                                  fill_mode='nearest')\n",
    "img_valid_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = img_train_gen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)\n",
    "validation_generator = img_valid_gen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras VGG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From F:\\Anaconda3\\envs\\MLtraining\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From F:\\Anaconda3\\envs\\MLtraining\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 48, 48, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 256)         295168    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 6, 6, 256)         590080    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                147520    \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 390       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 1,328,102\n",
      "Trainable params: 1,325,926\n",
      "Non-trainable params: 2,176\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding = 'same', kernel_initializer=\"he_normal\",\n",
    "                 input_shape = (img_rows, img_cols, 1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3), padding = \"same\", kernel_initializer=\"he_normal\", \n",
    "                 input_shape = (img_rows, img_cols, 1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #2: second CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #3: third CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #4: third CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #5: first set of FC => RELU layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Block #6: second set of FC => RELU layers\n",
    "model.add(Dense(64, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Block #7: softmax classifier\n",
    "model.add(Dense(num_classes, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From F:\\Anaconda3\\envs\\MLtraining\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      "1767/1767 [==============================] - 204s 115ms/step - loss: 1.9581 - acc: 0.2097 - val_loss: 1.7403 - val_acc: 0.2514- loss: 2.7352 - ETA: 3:32 - lo - ETA: 3:12 - loss: 2.6651 - ETA: 3:04 - loss: 2.6243 - acc: - ETA: 3:00 - loss: - ETA: 2:51 - loss: 2.5928 - acc: 0.17 - ETA: 2:50 - loss: 2.588 - ETA: 2:45 - loss: 2.5736 - a - ETA: 2:42 - loss: 2.5638 - acc - - ETA: 2:16 - loss: 2 - ETA: 2:13 - loss: 2.4171 - acc: 0.184 - ETA: 2:13 - loss: 2.4166 - acc: 0 - ETA: 2:12 - loss: 2 - ETA: 2:10 - loss: 2.3959 - acc: 0.1 - ETA: 2:09 - loss: 2.3913 - acc: 0.184 - ETA: 2:09 - loss: 2.3902 - ETA: 2:08 - loss: 2.3716 - ETA: 2:06 - loss: 2.3600 - acc: 0.18 - ETA: 2:06 - loss: 2.3597 - acc: 0.18 - ETA: 2:01 - loss: 2.3280 - ETA: 1:59 - loss: 2.3163 -  - ETA: 1:58 - loss:  - ETA: 1:5 - ETA: 1:52 - loss: 2.2780 - acc: 0.1 - ETA: 1:5 - ETA: 1:49 - loss: 2.26 - ETA: 1:47 - loss: 2.2524 - acc: 0. - ETA: 1:47 - loss: 2.2 - ETA: 1:45 - loss: 2.2414 - acc: 0 - ETA: 1:45 - loss: 2.239 - ETA: 1:43 - loss: 2.2321 - acc:  - ETA: 1:43 - loss: 2.2283 - acc: 0.1 - ETA: 1:43 - loss: 2.2261 - acc:  - ETA: 1:42 - loss: 2.2 - ETA: 1:37  - ETA: 1:34 - loss: - ETA: 1:32 - ETA: 1:29 - loss: 2.165 - ETA: 1:28 - loss: 2.1602 - acc:  - ETA: 1:27 - los - ETA: 1:21 - loss: 2.1396 - acc: - ETA: 1:21 - loss: 2.1368 -  - E - ETA: 0s - loss: 1.9584 - acc: 0.2\n",
      "Epoch 2/50\n",
      "1767/1767 [==============================] - 367s 208ms/step - loss: 1.7459 - acc: 0.2484 - val_loss: 1.7330 - val_acc: 0.2601\n",
      "Epoch 3/50\n",
      "1767/1767 [==============================] - 158s 90ms/step - loss: 1.7350 - acc: 0.2587 - val_loss: 1.7073 - val_acc: 0.2897: 2:10 - loss: 1.7505 - a - ETA: 2:09 - loss: 1 - ETA: 2:07 - loss: - ETA: 2:06 - loss:  - ETA: 2:00 - loss: 1.7462 - acc - ETA: 1:59 - loss: 1.7452 - acc: 0.2 - ETA: 1 - ETA: 1:57 - loss: 1.7443 - ac - ETA: 1:56 - loss: 1.74 - ETA: 1:54 - loss: - ETA: 1:52 - loss: - ETA: 1:51 - - ETA: 1:49 - loss: 1.7440 - ac - ETA: 1:48 - l - ETA: 1:46 - loss: 1.7423 - acc:  - ETA: 1:45 - loss: 1.7421 - acc: - ETA: 1:45 - loss: 1.7415 - acc: 0.2 - ETA: 1: - ETA: 1:42 - loss: 1. - ETA: 1:41 - loss: 1.740 - ETA: 1:40 - loss: 1.7402 - acc: 0.25 - ETA: 1:37  - ETA: 1:34 - loss: 1.7397 - ETA: 1:33 - loss: 1.7395 - acc: - ETA: 1:33 - loss: 1.7391 - ETA: 1: - ETA: 1:26 - loss: 1.7 - ETA: 1:24 - loss: 1.7394 - - ETA: 1:23 - loss: 1.7385 - acc: 0.25 - ETA: 1:23 - loss:  - ETA: 1:21 - loss: 1.7 - ETA: - ETA: 13s -  - ETA: 12s - loss - ETA: 10s - loss: 1.7346 - acc - ETA - ETA: 6s - loss: 1.7354 - acc: 0. - ETA: 6s - loss: 1.735 - ETA: 4s - loss: 1.7355  - ETA: 3s - loss: 1.7353 - acc: 0.258 - - ETA: 0s - loss: 1.7350 - acc: 0\n",
      "Epoch 4/50\n",
      "1767/1767 [==============================] - 153s 87ms/step - loss: 1.6973 - acc: 0.2794 - val_loss: 1.6014 - val_acc: 0.3360.7392 - acc: 0.25 - ETA - ETA: 2:02 -  - ETA: 1:48 - loss: 1.7244 - ac - ETA: 1:48 - loss: 1.7247 - acc: - E - ETA: 1:44 - loss: 1.7237 - - ETA: 1:43 - loss: 1.7 - ETA: 1:42 - loss: 1.7218 - acc: 0. - ETA: 1:42 - loss: 1.7214 - acc - ETA: 1:41 -  - ETA: 1:39 - loss: 1.7201 - ac - ETA: 1:38 - loss: 1.720 - ETA: 1:37 - loss: 1.7194 - acc: 0.271 - ETA: 1:37 - loss: 1.7196  - ETA: 1:36 - ETA: 1:34 - loss: 1.7199 - acc - ETA: 1:33 - loss: 1.7198 - acc: - ETA: 1:32 - loss - ETA: 1:31 - loss - ETA: 1:29 - loss: 1.7184 - acc: 0.26 - E - ETA:  - ETA: 1:24 - loss: 1.7161 - - ETA: 1:23 - loss: 1.7158 - acc: 0.272 - - ETA: 5s - loss: 1.69 - ETA: 4s - loss: 1.6983 - acc: 0. - ETA: 0s - loss: 1.6973 - acc\n",
      "Epoch 5/50\n",
      "1767/1767 [==============================] - 154s 87ms/step - loss: 1.6290 - acc: 0.3203 - val_loss: 1.5956 - val_acc: 0.4070- loss: 1.6726 - acc: 0.287 - ETA: 2:03 - loss: 1.6 - ETA: 2:08  - E - ETA: 2:11 - los - ETA: 2:09 - loss: 1.6559 - acc: 0 - ETA: 2:04 - loss: 1.6629 - ETA: 2:02 - loss: 1.6593 - acc: 0 - - ETA: 1:59 - loss - ETA: - ETA: 1:51 - loss: 1.6542 -  - ETA: 1:51 - loss: 1.6537 - acc: - ETA: 1:50 - loss: 1.6537 - acc: 0.29 - ETA: 1:50 - loss: 1.6532 - acc: 0.29 - ETA: 1:50  - ETA: 1:48 - loss: 1.6491 - acc: 0.2 - ETA: 1:48 - loss: 1.6479 - acc: 0.2 - ETA: 1:47 - loss - ETA: 1:46 - loss: 1.646 - ETA: 1: - ETA - ETA: 19s - loss: 1.6354 - acc - ETA: 19s - loss:  - ETA: 16s - loss: 1.6345 - acc:  - ETA:  - ETA: 15s - loss: 1.6342 - acc: 0. - - ETA: 11s - loss - ETA: 10s - loss:  - ETA:  - ETA: 7s - loss: 1.6 - ETA - ETA: 3s - loss: 1.6303 - acc: 0.31 - ETA: 3s - loss: 1.6302  - ETA: 2s - loss: 1.6301 - acc: 0.319 - ETA: 2s - lo\n",
      "Epoch 6/50\n",
      "1767/1767 [==============================] - 156s 88ms/step - loss: 1.5510 - acc: 0.3694 - val_loss: 1.6421 - val_acc: 0.4125ETA: 2:27 - loss: 1.5896 - acc: 0.36 - ETA: 2:2 - ETA: 2:28 - loss: 1.5901 - ETA: 2:27 - loss: 1.590 - ETA: 2: - ETA: 2:17 - loss: 1.5799 - acc: - ETA: 2:16 - loss: 1.5811 - acc: 0.36 -  - ETA: 2:13 - loss: 1.5823 - acc:  - ETA: 2:12 - loss - ETA: 2:10  - ETA: 1:56 - loss: 1.5819 - acc:  - ETA: 1:56 - loss: 1.5834 - a - ETA: 1:54 - - ETA: 1:52 - loss: 1.5812 -  - ETA: 1:50 - loss: 1.5791  - ETA: 1:49 - loss: 1.5805 - ac - ETA: 1:48 - loss: 1.5805 - ac - ETA: 1:47 - loss:  - ET  - ETA: 8s - loss: 1.5523 - acc: 0.368 - ETA: 8s - loss: 1.552 - ETA: 3s - loss: 1.5519 - acc: - ETA: 2s - loss:  - ETA: 1s - loss: 1.5512 - acc: 0. - ETA: 0s - loss: 1.5509 - acc\n",
      "Epoch 7/50\n",
      "1767/1767 [==============================] - 152s 86ms/step - loss: 1.4855 - acc: 0.4042 - val_loss: 1.4523 - val_acc: 0.4409acc: 0. - ETA: 2:07 - loss: 1.5367 - - ETA: 2:08 - loss: 1.5373 - acc - - ETA: 2 - ETA: 2:04 - loss: - ETA: 2:02 - loss: 1.5293 - acc: 0.3 - ETA: 2:02 - loss: 1.5289 - a - ETA: 1:56 - loss: 1.5227 - ETA: 1:56 - loss: 1.5 - ETA: 1:54 - - ETA: 1:50 - loss: 1.5201 - - ETA: 1:50 - loss:  - ETA: 1:48 - loss: 1.5178 - ETA: - ETA: 1:46 - loss: 1 - ETA: 1:44 - loss: 1.5207 - acc: 0.3 - ETA: 1:44 - loss: 1.5211 - acc: 0.3 - ETA: 1:43 - loss: 1.52  - ETA: 1:38 - loss: 1.5198 - acc - ETA: 1:38 - loss: 1.5186 - ETA: 1:36 - ETA: 1:34 - loss - ETA: 1:25 - loss: 1. - ETA: 1:23 - lo - ETA: 13s - lo - ETA: 1s - loss: 1.4857 \n",
      "Epoch 8/50\n",
      "1767/1767 [==============================] - 151s 86ms/step - loss: 1.4447 - acc: 0.4249 - val_loss: 1.4407 - val_acc: 0.4540717 - acc - ETA: 2:03 - loss: 1.4699 - ac - ETA: 2:02 - loss: 1.4671 - acc: 0.4 - ETA: 2:02 - lo - ETA: 2:01 - loss: 1.46 - ETA: 2:00 - loss: 1.4617 - acc - ETA: 2:00 - loss: 1. - ETA: 1:58 - loss: 1.4589 - acc: 0.419 - ETA: 1:58 - loss: 1.458 - ETA: 1:57 - loss - ETA: 1:56 - loss: 1.457 - ETA: 1:55  - ETA: 1:53 - loss: 1.4615 - ETA: 1:51 - los - ETA: 1:50 - lo - ETA: 1:42 - loss:  - ETA: 1:40 - loss: 1.4631 - acc: 0.414 - ETA: 1:40 - loss: 1.4627 - acc: 0.415 - ETA - ETA: 1:37 - loss: 1.4577  - ETA: 1:36 - loss: 1.4581 - acc: 0.41 - ETA: 1:36 - loss: 1.4572 - a - ETA: 1:35 -  - ETA: 1:33 - loss: 1.4572 - acc: 0.4 - ETA: 1:33 - loss: 1.4580 - acc: 0.41 - ETA: 1:33 - loss:  - ETA: 1:28 -  - ETA: 1:26 - los - ETA: - ETA: 1:19 - loss: 1.4555 - acc: 0 - ET - ETA: 1: - ETA: 29s - loss: 1. - ETA: 25s - loss:  - ETA: 12s - loss: 1.4477 - - ETA: 12s - loss: 1.4480 - a - ETA: 10s  - ETA: 7s - loss: 1.4467  - ETA: 6s - loss: 1.4461  - ETA: 5s - loss: 1.4460 - acc: 0. - ETA: 5s - \n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1767/1767 [==============================] - 151s 86ms/step - loss: 1.4156 - acc: 0.4365 - val_loss: 1.4589 - val_acc: 0.4650A: 2:04 - los - ETA: 2:06 - loss:  - ETA: 2:07 - los - ETA: 2:06 - ETA: 2:05 - loss:  - ETA: 2:04 - loss: 1.4405 - acc: 0. - ETA: 2:04 - los - - ETA: 1:40 - loss: 1.4230 - acc: 0.43 - ETA: 1:40 - lo - ETA: 1:38 - loss: 1.4251 -  - ETA: 1:37 - lo\n",
      "Epoch 10/50\n",
      "1767/1767 [==============================] - 149s 84ms/step - loss: 1.3892 - acc: 0.4505 - val_loss: 1.4727 - val_acc: 0.4517A: 2:04 - loss: - ETA: 2:04 - loss: 1.3998 - - ETA: 2:03 - loss: 1.4001 - acc:  - ETA: 2:03 - loss: 1.3976 - acc:  - ETA: 2:00 - loss: 1.3973 - acc: - ETA: 2:00 - loss: 1.3933 - acc: 0.45 - - ETA: 1:58 - loss: 1.3877 - ETA: 1:57 - loss: 1.3864 - acc - ETA: 1:57 - loss:  - ETA: 6s - loss: 1.3907 - a - ETA: 5s - loss: 1.3905 - acc: 0. - ETA: 5s - loss: 1.3904  - ETA: 3s - ETA: 1s - loss: 1.3881 - acc - ETA: 0s - loss: 1.3884 - a\n",
      "Epoch 11/50\n",
      "1767/1767 [==============================] - 150s 85ms/step - loss: 1.3675 - acc: 0.4597 - val_loss: 1.4140 - val_acc: 0.4366 - ETA: 2:07 - loss: 1.3683 - acc: 0.46 - ETA: 2:07 - loss: 1 - ETA: 2:05 - loss: 1.3 - ETA: 2:02 - loss: 1.3594 - acc: 0.465 - ETA: 1:43 - loss - ETA: 1:41 - loss: 1.3665 - - ETA: 1:40 - lo - ETA: 1:38 - loss:  - ETA: 1:37 - loss: 1.3655 - ETA: 1:36 - loss: 1. - ETA: 1:28 - loss: 1.3659 - acc: 0.461 - ETA: 1:28 - loss: 1.3658  - ETA: 1 - ETA: 1:21 - loss: 1.3670 - acc: - ETA: 22s - los - ETA: 6s - loss: 1.3669 - a - ETA: 5s - loss: 1.3664 - acc: 0.4 - ETA: 5s - loss: 1.3663 - acc: 0.46 - ETA: 5s - l - ETA: 2s - loss: - ETA: 1s - loss: 1.366\n",
      "Epoch 12/50\n",
      "1767/1767 [==============================] - 149s 84ms/step - loss: 1.3540 - acc: 0.4698 - val_loss: 1.4161 - val_acc: 0.4611 1.3707 - acc:  - ETA: 2:04 - los - ETA: 2:03 - loss: 1.3721 - acc: 0.4 - ETA: 2: - ETA: 2:01 - loss: 1.3743 - acc: 0.455 - ETA: 2:01 - loss:  - ETA: 2:00 - loss: 1.3 - ETA: 2:00 - loss: 1.3723  - ETA: 1:59 - loss: 1.3689 -  - ETA: 1  - E - ETA: 1:44 - loss: 1 - ETA: 1:43 - - ETA: 1:41 - loss: 1.3613 - acc: 0. - ETA: 1:40 - loss: 1.3618 - ETA: 1:39 - loss: 1.3620 - a - ETA: 1:38 - loss:  - ETA: 1:34 - loss: 1.3605 - a -  - ETA: 1:30 - loss: - ETA: 1:28 - loss: 1.3612 - acc: 0.4  - ETA: 1:25 - loss: 1.3603  - ETA: 1:23 - loss: 1.35 - ETA: 1 - ETA: 10s - loss: 1.3530 - a  - ETA: 4s - loss: 1.3526 - \n",
      "Epoch 13/50\n",
      "1767/1767 [==============================] - 150s 85ms/step - loss: 1.3398 - acc: 0.4763 - val_loss: 1.4195 - val_acc: 0.4835- ETA: 2:04 - loss: 1.3300 - acc:  - ETA: 2:06 - loss: 1.3538 - acc: - ETA: 2:06 - loss: 1.3 - ETA: 2:0 - ETA: 2:03 - loss: 1.3554 - acc: 0 - ETA: 2:03 - loss: 1.3556 - ETA: 2:02 - loss: 1.3551 - - ETA: 2:01 - loss: 1.3570 - acc: - ETA: 2:01 - loss: 1.3551 - acc - ETA: 2 - ETA: 1:46 -  - ETA: 1:44 - loss: 1.3532 - acc: 0.474 - ETA: 1:44 - loss: 1.3542 - acc: 0. - ETA: 1:43 - loss: 1.3524 - acc: 0.47 - ETA: 1:43 - loss: 1.3529 - acc: 0. - - ETA: 1:40 - loss:  - ETA: 1:38 - loss: - ETA: 1:33 - loss: 1.3469 - acc: 0.478 - ETA: 1:33 - los - ETA: 1:32 - loss: 1.3465 - acc: 0.47 - ETA - ETA: 1:29 - loss: 1.3485 - a - ETA: 1:28 - loss: 1.3482 - acc: - ETA: 1:27 - loss: 1.3473 - acc: 0.4 - ETA: 1:27 - loss: 1.3468 - acc: 0 - ETA: 1 - ETA: 1:24 - loss: 1.3484 - acc - ETA: 1:24 - loss: 1.3475 - acc: - ETA: 1:23 - loss: 1.3474 - acc:  - ETA: 1:23 - loss: 1.3472 - ETA: 1:09 - loss: 1.3467 - acc: 0.4 - ETA: 1:08 - loss: 1.3464 - acc: - ETA: 1:08 - loss: 1.3461 - acc: 0.47 - ETA:  - E - ETA: 1:02 - los - ETA: 1:00 - loss: 1.3445 - acc: 0.47 - ETA: 1:00 - loss: 1.3446 - acc: 0.4 - ETA: 1:00 - - E - ETA: 57s  - ETA - ETA: 35 - ETA: 33s -  - ETA:  - ETA: 9s - loss: 1.3405 - acc: 0.4 - ETA: 9s - loss: 1.3405 - a - ETA: 8s - loss: - ETA: 7s - loss: 1.3404 - ETA: 5s - loss: 1.3396 - - ETA: 4s - loss: 1.3393 - a - ETA: 4s - loss: 1.3394 - a - ETA: 3s - loss: 1.3387  - ETA: 2s - \n",
      "Epoch 14/50\n",
      "1767/1767 [==============================] - 150s 85ms/step - loss: 1.3298 - acc: 0.4832 - val_loss: 1.3862 - val_acc: 0.4955 loss: 1.3794 - a - ETA: 2:02 - - ETA - ETA: 1:20 - loss: 1.3376 -  - ETA: 1:19 - loss: 1.3383 - acc - ETA: 1:18 -  - ETA: 1:16 - loss: 1.3350 - acc: 0.48 - ETA: 1:16 - loss: 1.3349 - a - ETA: 1:15 - loss: 1.3354 - acc: 0 - ETA: 1:15 - loss: 1.3348 - acc: 0.481 - ETA: 1:14 - lo - ETA: 18s - loss: 1.3296 - ETA: 18s - loss:  - ETA: 17s - loss: 1.3300 - acc - ETA: 16s - loss: 1.3299 - acc: 0. - ETA: 16s - loss: 1.3301 - - ETA: 16s  - ET -  - ETA: 4s - loss: 1.33 - ETA: 3s -  - ETA: 0s - loss: 1.3305 - \n",
      "Epoch 15/50\n",
      "1767/1767 [==============================] - 148s 84ms/step - loss: 1.3130 - acc: 0.4877 - val_loss: 1.4015 - val_acc: 0.49601.3059 - acc: 0. - ETA: 2:07 - los - ETA - ETA: 2:00 - loss: 1.3258 - acc: 0. - ETA: 1:59 - loss: 1.3254 - acc: 0.4 - ETA: 1:59 - loss: 1.3256 - acc: 0.48 - ETA: 1:59 - loss: 1.3249 - acc: 0.48 - ETA: 1:59 - loss: 1.32 - ETA: 1:58 - loss: 1.3209 - a - ETA: 1:57 - loss: 1.3230 - - ETA: - ETA: 1:54 - loss: 1.3199 - acc: 0 - ETA: 1:54 - loss: 1.3210 - acc - ETA: 1:53 - loss: 1.3192  - ETA: 1:52 - loss: 1.320 - ETA: 1:51 - loss: 1.3 - ETA: 1:50 - loss: 1. - ETA: 1:45 - loss: 1.3163 - acc: 0 - ETA: 1:45 - loss: 1.3168 - - ETA: 1:44 - loss: 1.3150 - - ETA: 1:43 - loss: 1.3145 - acc:  - ETA: 1:43 - loss:  - ETA: 1:41 - loss: 1.3176 - acc: 0.48 - ETA: 1:41 - loss - ETA: 1:39 - loss: - ETA: 1:38 - loss: 1.316 - ETA: 1:36 -  - ETA: 1:34 - loss: 1.3132 - acc: 0 - ETA: 1:34 - loss: 1.3124 - acc: 0.48 - ETA: 1:30 - loss: 1.3095 - a - ETA: 1:30 - loss: 1.3089 - ETA: 1:29 - loss: 1.3094 - acc: 0. - ETA: 1:28 - loss: 1.3100 - ac - ETA: 1:28 - loss: 1.3100   -  - ETA: 1:10 - l - ETA: 1:08 - loss: - ETA: 1:03 - loss: 1.3130 - acc:  - ETA: 1:02 - loss: 1.3126 - ETA: 1:01 - loss: 1.3126 - acc:  - ETA: 29s - loss: 1.3138 - a - E - ETA: 27s -  - ETA: 26s - loss:  - E - ETA: 12s - l - ETA: 7s - loss: 1.3149 - acc: 0.486 - ETA: 7s - loss: 1.3147 - acc: - ETA: 6s - loss: 1.3152 - a - ETA: 5s - loss: 1.3147 - ac - ETA: 4s - loss: 1.3156 - acc: 0.486 - ETA: 4s - loss: 1.3154 - - ETA: 3s - loss: 1.3151 - acc: 0.486 - ETA: 3s - loss: 1.3150 - acc:  - E - ETA: 0s - loss: 1.3137 - acc: 0\n",
      "Epoch 16/50\n",
      "1767/1767 [==============================] - 149s 84ms/step - loss: 1.3097 - acc: 0.4868 - val_loss: 1.4409 - val_acc: 0.50882:04 - loss: 1.2935 - acc: - ETA: 2:05 - loss: 1 - ETA: 2:04 - loss: - ETA: 2:04 - loss: 1.3031 - ET - ETA: 1:49 - loss: 1.3036  - ETA: 1:48 - loss: 1.3025 - acc:  -    - ETA - ETA: 1:29  - ETA - ETA: 1:24 - loss: 1.3072 - acc: 0.49 -  - ETA: 1:21 - loss: 1.3072 -  - ETA: 1:20 - loss: 1.3072 - acc:  - ETA: 1:20 - loss: 1.3079 - acc: 0.491 - ETA: 1 - ETA: 44s - loss: 1. - ETA: 43s - loss: 1.3072 - acc: 0.48 - E - ETA: 18s - loss: 1.3079 - acc:  - ETA: 18s - loss - ETA:  - ETA: 11 - ETA: 10s - loss: 1.3102 -  - ETA: 9s - loss: 1.3107 - acc: 0.486 - ETA: 9s - loss: - ETA: 8s - loss: 1.3105 - acc: 0.4 - ETA: 7s - loss: 1.3103 - acc - ETA: 7s - l - ETA: 5s - loss: 1.3088 -  - ETA: 4s - loss: 1.3083 - a - ETA: 3s - l - ETA: 1s - loss: 1.309\n",
      "Epoch 17/50\n",
      "1767/1767 [==============================] - 148s 84ms/step - loss: 1.3030 - acc: 0.4918 - val_loss: 1.4519 - val_acc: 0.4812120 - acc - ETA: 1:52 - loss: 1.3178 - ac - ETA: 1:56 - loss: - ETA: 2:02 -  - ETA: 2:04 - loss: 1 - ETA: 2:03 - loss: 1.3150 - acc: - ETA: 2:03 - loss: 1.314 - ETA: 2:03 - loss: 1.3153 - acc: 0.48 - ETA: 2:03 - loss: - ETA: 1s - loss: 1.3033 - - ETA: 0s - loss: 1.3037 - acc:  - ETA: 0s - loss: 1.3031 - acc: 0.\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1767/1767 [==============================] - 149s 84ms/step - loss: 1.2697 - acc: 0.5081 - val_loss: 1.3623 - val_acc: 0.5190 - ETA: 2:00 - loss: 1.2899 - a - ETA: 2:00 - loss: 1.2868  - ETA: 1:59 - l - ETA: 1:58 - loss: 1.2930 - acc: 0.503 - ETA: 1:55 - loss: 1.2873 - acc: 0. - ETA: 1:54  - ETA: 1:53 - loss: 1.2822 - ETA: 1:52 - loss: 1.2857 - - ETA: 1:51 - loss: 1.28 - ETA: 1:49 - loss: 1.2891 - ETA: 1:48 - loss: 1.28 - ETA: 1:47 -  - ETA: 1:45 - loss: 1. - ETA: 1:44 - loss: 1.2860 - ac - ETA: 1:43 - loss: 1.2851 - acc: - ETA: 1:43 - loss: 1.2852 - a - ETA: 1:42 - loss: 1.28 - ETA: 1:41 - loss: 1.28 - ETA: 1:39 - loss: 1.2873 - acc: 0.503 - ETA: 1:39 - loss: 1.2871 -  - ETA: 1:35 - loss: 1.2856 - acc: 0. - ETA: 1: - ETA: 1:29 - loss: - ETA: 1:27 - loss: 1. - ETA: 1:26 - loss - ETA: 1:24 - loss: 1.2794 - acc: 0. - ETA: 1:24  - ETA: 1:22  - ETA: 1:16 - loss: 1.2799 - acc: 0.506 - ETA: 1:16 - loss: 1.2795 - acc: 0.50 - ETA: 1:16 - loss: 1.2796 - acc: - ETA: 25s -  - ETA: 24s - loss: 1.2727 - - ETA: 23s - lo - ETA: 22s - loss: 1.2717 - acc: 0. - - E - ETA: 13s - loss: 1.2710 - acc: 0. - ETA: 12s - loss: 1.2710 - acc: 0.50 - ETA: 12s - loss: 1.2710 - acc: 0. - ETA:  - ETA: 9s - - ETA: 7s - loss: 1.2711 - - ETA: 6s - lo - ETA: 1s - loss: 1.2694 \n",
      "Epoch 19/50\n",
      "1767/1767 [==============================] - 149s 84ms/step - loss: 1.2637 - acc: 0.5158 - val_loss: 1.3896 - val_acc: 0.518202 - loss: 1.3145 -  - ETA: 2:03 - loss: 1.3087 - acc: 0 - ETA: 2:04 - loss: 1.3041 - acc: 0. - ETA: 2:04 - loss: 1.3009 - acc: 0 - E - ETA: 2:06 - loss:  - ETA: 2:05 - loss: 1 - ETA: 2:05 - loss: 1.2841  - ETA: 2:04 - loss: 1.2832 - acc:  - ETA: 2:03 - loss: 1.2890 - acc:  - ETA: 2:03 - loss: 1.2899 - acc: 0.50 - ETA: 2:03 - loss: 1.2915 - ac - ETA: 2:03 - loss: 1.2878 - acc: 0.509 - ETA - ETA: 2:00 - loss: 1.2895 - acc - ETA: 2:00 - loss: 1.2868 - acc: 0.510  - ETA: 1:57 - l - ETA: 1:55 - loss: - ETA: 1:54 - loss: 1.2634 -  - ETA: 1:53 - loss: 1.2619 - acc: 0.519 - ETA: 1:53 - loss: 1.26 - ETA: 1:52 - los - ETA: 1:47 - loss: 1.2533 - acc: 0.5 - - - ETA: 7s - loss: 1.2623 - acc:  - ETA - ETA: 4s - los - ETA: 2s - loss: 1.2642 - - ETA: 1s - loss: 1.26\n",
      "Epoch 20/50\n",
      "1767/1767 [==============================] - 148s 84ms/step - loss: 1.2564 - acc: 0.5144 - val_loss: 1.3898 - val_acc: 0.525949 - loss: 1.2474 - a - ETA: 1:54 - loss: 1 - ETA: 1:59 - loss: 1.2496 - acc: 0. - ETA: 2:00 - l - ETA: 2:04 - loss:  - ETA: 2:03 - loss: 1.266  - ETA: 2:01 - loss: 1.2555 - a - ETA: 2:01 - loss: 1.2588 - acc: 0 - ET - ETA: 1:58 - loss: 1.2642 - acc: 0.5 - ETA: 1:58 - loss: 1.2633 - acc: 0.5 - ETA: 1:58 - loss: - ETA:  - ETA: 1:54 - loss: 1.2694 - acc: 0 - ETA: 1:54 - loss: 1.2690 - acc: - ETA: 1:53 - loss: 1.2679  - ETA: 1:53 - loss: 1.2646 - acc: - ETA: 1:5 - ETA: 1:50 - los - ETA: 1:48 - los - ETA: 1:47 - loss: 1 - ETA: 1:43 - loss: 1.2624 - acc: 0 - ETA: 1:42 - loss: 1.2626 - acc: 0.51 - ETA: 1:42 - loss: 1.262 - ETA: 1:41 - loss: 1.2619 - ETA: 1:40 - loss: 1.2631 - acc: 0.515 - ETA: 1:40 - loss: 1.2624 - acc: 0. - ETA: 1:39 - loss: 1.2612 - - ETA: 1:38 - loss: 1.2611 - - ETA: 1:37 - loss: 1.2 - ETA: 1:36 - loss: 1.2595 - acc - ETA: 1:35 - loss: 1. - ETA: 1:3 - ETA: 1:32 - loss: 1.2582 - ac - ETA: 1:31 - loss: 1.2585 - - ETA: 1: - ETA: 1:24 -  - ETA: 1:22 - loss: 1.2608 - acc - ETA: 1:22 - loss: 1 - ETA: 1:20 - loss: 1.2600 - acc: 0 - ETA: 1:20 - loss: 1.25 - ETA: 1:18 - loss: 1.2601   - ETA: 1:14 - lo - ETA: 17 - ETA: 14 - ETA: - ETA: 6s - loss: - ETA: 4s - loss: 1.25 - ET - ETA: 0s - loss: 1.2563 - a\n",
      "Epoch 21/50\n",
      "1767/1767 [==============================] - 148s 84ms/step - loss: 1.2510 - acc: 0.5170 - val_loss: 1.3798 - val_acc: 0.5250ETA: 2:04 - loss: 1.2367 - acc: 0 - ETA: 2:04 - lo - ETA: 2:02 - loss: 1.2337 - acc - ETA: 2:02 - loss - ET - ETA: 1:58 - loss: 1.2424 - acc: 0 - ETA: 1:58 - loss: 1.2 - ETA: 1:57 - loss: 1.2477  - ETA: 1:56 - lo - ETA: 1:48 - loss: 1.2 - ETA: 1:43 - loss: 1.2 - ETA: 1:42 - loss: 1. - ETA: 1:40 - loss: 1.2314 - - ETA: 1: - ETA: 1:34 - loss: 1.2350 - acc: 0.52 - ETA: 1:34 - lo - ETA: 1:3 - ETA: 1:26 - loss: 1.2441 - ac - ETA: 1:25 - lo - ETA: 1:23 - loss: 1.2452 - acc: 0.518 - ETA: 1:23 - loss: 1.2450 - acc:  - ETA: 1:23 - loss: 1.2455 - - ETA: 1:22 - loss - - ETA: 1:17 - l - ETA: 1:15  - ETA: 1:09 - loss: 1.2457 - acc:  - ETA: 1: - ETA: 1:07 - loss: 1.2447  - ETA: 1:02 - loss: 1.248 - ETA: 55s - loss: 1.2499 - a - ETA: 55s - loss: 1.2494 - acc: 0. - ETA: 55s -  - ETA: 27s - loss: 1. - ETA: 27s - loss: 1.2505 - - ETA: 26s - loss: 1.2507 - acc: 0.51 - ETA: 26s - loss:  - ETA: 25s -  - ETA: 24s - loss: 1.25 - ETA: 22s - loss: 1.2512 - - ETA:  - ETA: 20s - loss: 1.2515 - acc:  - ETA: 17s - loss: 1.2524 - - ETA: 13s - loss: 1. - ETA: 11 - ETA: 9s - loss: 1.2515 - acc:  - ETA: 9s - loss: 1.2511 - acc: - ETA: 8s - loss: 1.2511 - acc: 0.51 - ETA: 8s - loss: 1.2508 - acc: - ETA: 7s - loss: 1.2507 - acc: 0. - ETA: 7s - loss: 1.2506 - acc: - ETA: 6s - ETA: 4s - loss: 1.2516 - a - ETA: 3s - loss: 1 - ETA: 2s - loss - ETA: 0s - loss: 1.2508 - acc: 0\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "Epoch 22/50\n",
      "1767/1767 [==============================] - 149s 84ms/step - loss: 1.2369 - acc: 0.5245 - val_loss: 1.3397 - val_acc: 0.5276 loss: 1.2535 - acc: 0. - ETA: 2:05 - loss: 1.2528 - ac - ETA: 2:05 - loss: 1.2589 - ac - ETA: 2:05 - loss - ETA: 2:04 - loss: 1.2486 - acc: - ETA: 2:04 - l - ETA: 2:03 - loss: 1.2462 -  - ETA: 2:02 - loss: 1.2431 - acc: 0.5 - ETA: 2:00 - loss: 1.2417 - a - ETA: 1:59 - loss: 1.24 - ETA: 1:58 - loss:  - ETA: 1:57 - loss: 1.2444 - acc: 0 - ETA: 1:56 - l - ETA: 1:52 - loss: 1.237 - ET - ETA: 1:48 - loss: 1. - ETA: 1:47 - loss: 1.2344 - acc: - ETA: 1:46 - loss: - ETA: 1:45 - loss: 1.2362 - acc: 0.5 - ETA: 1:44 - loss: 1.2372 - ETA: 8s - loss: 1.2368 - -  - ETA: 1s - loss: 1.23\n",
      "Epoch 23/50\n",
      "1767/1767 [==============================] - 149s 84ms/step - loss: 1.2415 - acc: 0.5221 - val_loss: 1.3607 - val_acc: 0.52734 - loss: 1.2792 - acc: 0.507 - ETA: 2:04 - loss: 1.286 - ETA: 2:06 - loss: 1.2818 - a - ETA: 2:07 - loss: 1.2816 - acc: 0. - ETA: 2:07 - ETA: 2:06 - loss: 1 - ETA: 2:06 - lo - ETA: 2:06 - loss: 1.263 - ETA: 2:05 - loss: 1.2613 - acc: 0.5 - ETA: 2:05 - loss: 1.2626 - - ETA: 2:04 - loss: 1.2637 - ETA: 2:03 - loss: - ETA: 2:02 - loss: 1.2646 - acc: 0.5 - ETA: 2:02 - loss: 1.2628 - acc: - ETA: 2:01 - loss: 1.2610 - acc: 0. - ETA: 2:01 - lo - ETA: 1:59 - loss: 1.2625 - acc:  - ETA: 1:56 - loss: 1.2574 - acc: 0.514 - ETA: 1:56 - loss: 1.2574 - acc - ETA: 1:56 - loss: - ETA: 1:54 - loss: 1.2537 - acc:  - ETA: 1:51 - loss: 1.2543 - acc: 0. - ETA: 1:47 - lo - ETA: 1:42 - - ETA: 1:36 - loss: 1.2516 - acc: 0 - ETA: 1:36 - ETA: 1:27 - loss: 1.2504 - acc: 0.51 - E - ETA: 1:24 - loss: 1 - ETA: 1:22 -  - ETA: 1:20 - loss: 1.2521 -  - ETA: 1:19 - loss: 1.2511 - acc:   - ETA: 1:16 - los - ETA: 1:14 - - ETA: 42s - loss: 1.2477 - acc - ETA:  - ETA: 16s - loss: 1.2429 - acc - ETA: 12s - loss: 1.2422 - - ETA: 10s - loss: 1.2423 - - ETA: 10s - loss: 1.2424 - a - ETA: 9s - ETA: 7s - loss: 1.241 - ETA: 6s - loss: 1.2413 - acc: 0.52 - ETA: 6s - loss: 1.2413 - acc - ETA: 5s - loss: 1.2410  - ETA: 4s - loss: 1. - ET\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1767/1767 [==============================] - 148s 84ms/step - loss: 1.2365 - acc: 0.5203 - val_loss: 1.3797 - val_acc: 0.52132:01 - loss: 1.1716  - ETA:  - ET - ETA: 2:04 - loss: 1. - ETA: 2:04 - loss: 1.2129 - a - ETA: 2:03 - loss: 1.2065 - ETA: 2:01 - loss - ETA: 1:59 - loss: 1.2191 - acc: 0 - ETA: 1:59 - loss: 1.2176 - acc - ETA: 1:59 - loss: 1.2213 - acc: 0. - ETA: 1:59 - loss - ET - ETA: 1:52 - loss: 1.2251 - acc: 0.525 - E - E - ETA: 1:46 - loss: 1.2184 - acc: 0.5 - ETA: 1:46 - loss: 1.2181 - acc - ETA - ETA: 1:36 - loss: 1.22 - ETA: 1:35 - loss:  - ETA: 1:30 - lo - ETA: 1:28 - loss: 1.2319 - acc: 0. - ETA: 1:28 - loss: 1.2 - ETA: 1:26 - loss: 1.2303 - acc: 0.524 - ETA: 1:26 - loss: 1.2308 - acc: 0. - ETA: 1:26 - loss: 1 - ETA: 1:24 - loss: 1.2335 - acc:  - ETA: 1:21 - loss: 1. - ETA: 1:19 - loss: - ETA: 1:17 - loss: - ETA:  - E - ETA: 9s - loss: 1.2367 - acc: 0.51 - ETA: 8s - loss: 1.2368 - acc: 0 - ETA: 8s - loss: 1.2366 - acc: 0.518  - ETA: 5s - loss: 1.2369 - acc:  - ETA: 4s - loss: 1.2369 - acc: 0.51 - ETA: 4s - loss: 1.2367 - a - ETA: 3s -  - ETA: 1s - loss: 1\n",
      "Epoch 25/50\n",
      "1767/1767 [==============================] - 148s 84ms/step - loss: 1.2379 - acc: 0.5241 - val_loss: 1.3693 - val_acc: 0.5239TA: 2:07 - loss: 1.2119 - acc: 0.5 - ETA: 2:07 - loss: 1.2 - ETA:  - ETA: 2:06 - loss: 1.2228 - acc: 0.53 - ETA: 2:06 - loss: 1.2219 - acc: - ETA: 2:06 - lo - ETA: 1 - ETA: 1:51 - loss: 1.2240 - acc: 0.533 - ETA:  - ETA: 1:48 - loss: 1.2294 - acc: 0. - ETA: 1:48 - loss: 1.2309 - acc: - ETA: 1:48 - loss: 1.2335 -  - ETA: - ETA: 6s - loss: 1.2367 -  - ETA: 5s - loss: 1.2 - ETA: 4s - loss: 1.2372 - acc: - ET - ETA: 0s - loss: 1.2387 -\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "Epoch 26/50\n",
      "1767/1767 [==============================] - 149s 84ms/step - loss: 1.2298 - acc: 0.5251 - val_loss: 1.3591 - val_acc: 0.5296: 0.53 - ETA: 1:57 - loss: 1.2500 - acc: 0 - ETA: 1:58 - loss: 1.2419  - ETA: 2:02 - loss: 1.2423  - ETA: 2:04 - loss: 1.2581  - ETA: 2:05 - loss: 1. - ETA: 2:06 - loss: 1.2518 - acc: 0.5 - ETA: 2 - ETA: 2:04 - loss: 1.2401 - acc: 0 - ETA: 2:03 - loss: 1.2470 - acc: 0.524 - ETA: 2:03 - loss: 1. - ETA: 2:03 - loss: 1 - ETA: 2:01 - loss: 1.2452 - acc: - ETA: 2:01 - loss: 1.2431 - acc: 0.524 - ETA: 2:01 - loss: 1.2435 - acc: 0. - ETA: 2: - ETA: 1:59 - ETA: 1:57 - loss: 1.2450 - ac - ETA: 1:56 - loss: 1. - ETA: 1:55 - loss: 1.2332 - acc: 0.52 - ETA: 1:55 - loss: 1.2328 -  - ETA: 1:54 - loss: 1.2345 - acc: 0. -  - ETA: 1:51 - loss: 1.2339 - acc: 0.5 - ETA: 1:51 - loss: 1.233 - ETA: 1:50 - lo - ETA: 1:48 - loss: 1.2306 - a - ETA: 1:47 - loss: 1. - ETA: 1:45 - loss: - ETA: 1:31 - loss: 1.2294 - ac - ETA: 1: - ETA: 24 - ETA: 4s - loss: 1.2280 - acc:  - ETA: 3s - loss - ETA: 1s - loss: 1.2291 - ac - ETA: 0s - loss: 1.2297 - acc: 0 - ETA: 0s - loss: 1.2295 - acc:\n",
      "Epoch 27/50\n",
      "1767/1767 [==============================] - 148s 84ms/step - loss: 1.2326 - acc: 0.5267 - val_loss: 1.3631 - val_acc: 0.5352:02 - loss: 1.2248 - ac - ETA: 2:02 - - ETA: 1:57 - loss: 1.2288  - ETA: 1:56 - loss: 1.2285 - acc: 0.522 - ETA: 1:56 - loss: 1.22 - ETA: 1:55 - loss: 1.2265 - acc: 0.523 - ETA: 1:55 - los - ETA: 1:53 - loss: 1.2253 - acc: - ETA: 1:53 - loss: 1.2256  - ETA: 1:52 - loss: 1.2238 - acc: 0 - ETA - ETA: 1: - ETA: 1:46 - loss: 1.229 - ETA: 1:45 - loss:  - ETA: 1:43 - loss: 1.2334 - acc: - ETA: 1:43 - loss: 1.2346 - ac - ETA: 1:42 - loss - ETA: 1:41 - loss: 1.2376 - acc: 0 - ETA: 1:30 - loss: 1.2392 - acc: 0 - ETA: 1:30 - loss: 1.2405 - ac - ETA: 1:29 - loss:  - ETA: 1:28 - loss: 1.239 - ETA: 1:27 - l - ETA: 1:25 - loss: 1.2383 - ac - ETA: 1:24 - loss: 1.2385 - acc: - ETA: 1:2 - ETA: 1:17 - loss: 1.2351 - ac  - ETA: 12s - loss: 1.2326 - - ETA: 12s - loss: 1.2328 - - ETA: 11s - loss: 1.2328 - a - ETA: 11s - loss: 1.2328 - - ETA: 8s - los - ETA: 3s - loss: 1.2338 - acc: 0 - ETA: 2s - loss: 1.2336 - a - ETA: 1s - loss: 1.23 - ETA: 0s - loss: 1.2330 - acc: 0.52 - ETA: 0s - loss: 1.2330 - acc: 0\n",
      "Epoch 28/50\n",
      "1767/1767 [==============================] - 148s 84ms/step - loss: 1.2366 - acc: 0.5238 - val_loss: 1.3554 - val_acc: 0.5273 acc: 0. - ETA: 2:04 - loss: 1.2753 - acc: 0.50 - ETA: 2:05 - loss: 1.2697 - acc: - ETA: 2:05 - loss: 1.2687 -  - ETA: 1:55 - los - ETA: 1:51 - lo - ETA: 1:40 - loss: 1.2233 - acc: 0 - ETA: 1:39 - loss: 1.2230  - ETA: 1:38 - loss: 1.2234 -  - ETA: 1:37 - loss: 1.2216 - a - ETA: 1:37 - loss: 1.2222 - acc: 0.5 - ETA: 1:36 - loss: 1.2230 - ETA: 1:35 - loss: 1.2216 - acc: 0.5 - ETA: 1:35 - loss: 1.2217 - acc:  - ETA: 1:35 - loss: 1.2208 -  - ETA: 1:34 - loss: 1.2202 - acc:  - ETA: 1:33 - loss: 1.2224 - acc -  - ETA: 1:30 - loss: 1.2228 - acc: - ETA: 1:29 - los - ETA: 1:27 - loss: 1.2232 - acc - ETA: 1:17 - loss: 1.2271 - acc:  - ETA: 1:16 - loss: 1.2269 - acc: 0.5 - ETA: 1:16 - loss: 1.2276 - acc: 0 - ETA: 1:16 - loss: 1.2285 - acc - - ETA: 12s - loss:  - ETA - E - ETA: 6s - loss: 1.2364 - acc: 0 - ETA: 6s - loss: 1.2362 - acc: 0.523 - ETA: 6s - loss: 1.2363 - - ETA: 5s - loss: 1.2373 - - ETA: 4s - loss: 1.2374 - acc:  - ETA: 3s - loss: 1.2376 -  - \n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "Epoch 29/50\n",
      "1767/1767 [==============================] - 149s 84ms/step - loss: 1.2294 - acc: 0.5290 - val_loss: 1.3849 - val_acc: 0.5171- loss: 1.2362 - acc - ETA: 1:46  - ETA: 1:41 - lo - ETA: 1:40 - loss: - ETA: 1:38 - loss: 1.2341 - acc:  - ETA: 1:37 - loss: 1.2339  - ETA: 1:33  - ETA: 1:28 - ETA: 1:25 - loss: 1.22 - ETA: 1:24 - loss: 1.2283 - acc - - ETA: 1:10 - - ETA: 1:08 - loss: 1.2288 - a - ETA: 1:07 -  - ETA: 1:05 - loss: 1. - ETA: 1:04 - ETA: 1 - ETA: 59s - loss: 1. - ETA:  - ETA: 49s - loss: 1.2318 - - ETA: 48s  - ETA: 37s - loss: 1.2322 - a - ETA: 35s - loss: 1.2319 - acc: 0. - ETA: 35s - loss: 1.2318 - acc - ETA: 35s - loss: 1.23 - ETA: 34s - loss: 1.2315 - ETA: 34s - loss:  - ETA: 33 - ETA: 30s - loss: 1.2327 - acc: 0.52 - ETA: 30s - loss: 1.23 - - ETA: 15s - loss: 1.2296 - acc: 0. - ETA: 14s - loss: 1.2298 - acc:  - ETA: 14s - loss: 1. - ETA - ETA: 12 - ETA: - ETA: 9s - loss: 1.2291 - acc: 0.528 - ETA: 9s - loss - ETA: 7s - loss: 1\n",
      "Epoch 30/50\n",
      "1767/1767 [==============================] - 149s 84ms/step - loss: 1.2348 - acc: 0.5244 - val_loss: 1.3446 - val_acc: 0.53507 - loss: 1.2445 - acc: 0 - ETA: 1:47 - loss - ETA: 1:46 - loss: 1.2480 -  - ETA: 1:45 - l - ETA: 1:43 - loss: 1.2444 - a - ET - ETA: 20s - loss: 1.23 - ETA: 19s - loss: 1. - ETA: 14s - loss - ETA: 13s - loss: 1.2348 - acc: 0. - ETA: 11s - loss: 1.2350 - a - ETA: 10s - lo - ETA: 6s - loss: 1.2352 - acc: - ETA: 6s - loss: 1.2349  - ETA: 5s - loss: 1.2349 - acc: - ETA: 4s - loss: 1.234\n",
      "Epoch 31/50\n",
      "1767/1767 [==============================] - 148s 84ms/step - loss: 1.2328 - acc: 0.5257 - val_loss: 1.3448 - val_acc: 0.5264 2:05 - loss: 1.2445 - acc: - ETA: 2:05 - loss: 1.2371 - acc: 0.52 - ETA: 2:05 - lo - ET - ETA: 1:59 - loss: 1.216 - ETA: 1:57 - loss: 1.2133 - acc: 0 - ETA: 1:57 - loss: 1.21 - ETA: 1:56 - loss: 1.2093 - a - ETA: 1:55 - loss: 1.2111 -  - ETA: 1:55 - loss: - ETA: 1:50 - loss: 1.2176 - ETA: 1:49 - los - ETA: 1:22 - loss: 1.22 - ETA: 1:21 - loss: 1.2309 - acc: 0.52 - ETA: 1:20 - ETA: 1:18 - l - ETA: 1:16 - los - ETA: 1:14 - loss: 1.2312 - acc: 0.526 - ETA: 1:14 - loss: 1.2319 - acc - ETA: 1:13 - loss: 1.2323 - ac - ETA: 1:13 - - ETA: 1:10 - loss: 1.2310 - - ETA: 1:10 - loss: 1.2307 - acc: 0 - ETA: 1:09 - loss: 1.2302 - - ETA: 1:05 - ETA: 36s - loss: 1.2342 - acc - ETA: 36s - loss: 1.2337 - acc:  - ETA:  - ETA: 35 - ETA: 32s - loss: 1.2327 - acc: 0. - ETA: 32s - loss: 1.2328 - acc:  - ETA: 31s - loss: 1.2328 - acc - ETA: 31s - loss - ETA: 30s - loss:  - ETA: 20s - loss: 1.2325 - ETA: 19s - loss:  - ETA: 18s -  - - ETA: 14s - loss: 1.2336 - acc - ETA: 14s - loss:  - ETA: 13s - loss: 1.2340 - acc:  - ETA: 13s - loss: 1. - ETA: 10s - lo - ETA: 9s - loss: 1.2326 - ac - E - ETA: \n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1767/1767 [==============================] - 149s 84ms/step - loss: 1.2307 - acc: 0.5229 - val_loss: 1.3733 - val_acc: 0.5230 1:49 - loss: 1.2868 - acc: 0.494 - ETA: 1:50 - loss: 1.2853 - a - E - ETA: 2:04 - loss: 1.2297 - acc: 0. - ETA: 2:04 - loss: 1.2269 - acc: 0. - ETA: 2:04 - loss: 1.2234 - - ETA: 2:04 - loss: 1.2 - ETA: 2:03 - loss: 1.2265 - a - ETA: 1:57 - loss: 1.2259 - acc: 0.524 - ETA: 1:57 - loss: 1.2257 - ETA: 1:56 - loss: 1.  - E - ETA: 1:49 - loss: 1.2337 - - ETA: 1:48 - loss: 1. - ETA: 1:47 - loss: 1.2366 - acc: 0. - ETA: 1:44 - loss: 1.2382 - a - ETA: 1:43 - loss: 1.2391 - ETA: 1:42 - loss: 1.2 - ETA: 1:40 - loss: 1.2384 - a - ETA: 1:40 - loss: 1.236 - ETA: 1:29 - loss: 1.  - ETA: 1:21 - loss: 1.2305 - acc - ETA: 1:20 - loss: 1.2298 - a - ETA: 1:19 - los - - ETA: 32s - loss: 1.2271 - acc: 0.52 - ETA: 32s -  - E - ETA - ETA: 19s - loss: 1.2273 - acc: 0.52 - ETA: 19s - loss: 1.227 - ETA - ETA: 4s - loss:  - ETA: 2s - loss: 1.2301 - acc: 0.522 - ET\n",
      "Epoch 33/50\n",
      "1767/1767 [==============================] - 148s 84ms/step - loss: 1.2301 - acc: 0.5258 - val_loss: 1.3818 - val_acc: 0.52362:04 - loss: 1.2427 - ETA: 2:04 - loss: 1.2359 - ac - ETA: 1:54 - loss: 1.2368 - acc: 0.52 -  - ETA: 1:49 - loss: 1.2338 - acc: 0 - ETA: 1:48 - loss: 1.2325 - ETA: 1:47 - loss: 1.2329 - acc:  - ETA: 1 - ETA: 1:44 - loss: 1.2309 - acc: 0.52 - ETA: 1:44 - loss - ETA: 1:42 - loss: 1 - ETA: 1:38 - loss: 1.2349 - acc: 0.527 - ETA: 1:38 - loss: 1.2352 - acc: 0.527 - ETA: 1:38 - loss: 1.2347 - acc: - ETA: 1:37 - loss: 1.23 - ETA: 1:36 - loss: 1.2371 - acc: 0.5  - ETA: 32s - loss: 1.2316 - - ETA:  - ETA - ETA: 27s - loss - ETA: 0s - loss: 1.2298 - acc: - ETA: 0s - loss: 1.2302 - acc: 0.52\n",
      "Epoch 34/50\n",
      "1767/1767 [==============================] - 148s 84ms/step - loss: 1.2336 - acc: 0.5277 - val_loss: 1.3360 - val_acc: 0.52761.22 - ETA: 2:06 - loss: 1.2208 - acc: 0 - - ETA: 2:02 - loss: 1.2489 - acc - ETA: 2:02 - loss: 1.2479 - acc: 0.516 - ETA: 2:02 - loss: 1.2485 - acc: 0 - - ETA: 1:59 - loss: 1.2473  - ETA: 1:58 - loss: 1.2443 - a - ETA: 1:57 - loss - ETA: 1:50  - ETA: 1:48 - loss: 1.2532  - ETA: 1:47 - loss: 1.2519 - - ETA: 1:36 - loss: 1.2417 - E - ETA - ETA: 40s -  - \n",
      "Epoch 35/50\n",
      "1767/1767 [==============================] - 148s 84ms/step - loss: 1.2317 - acc: 0.5296 - val_loss: 1.3793 - val_acc: 0.521022 - ETA: 1:56 - loss: 1.2233 - ac - ETA: 1:59 - loss: 1.2309 - acc: 0.512 - ETA: 2:00 - loss: 1.2251 - acc: 0.5 - ETA: 2:00 - loss: 1.2334 -  - ETA: 2:02 - loss: 1.2161 - acc: 0.52 - ETA: 2:03 - loss: 1.225 - ETA: 2:0 - ETA: 2:06 - loss:  - - ETA: 2:00  - ETA: 1:58 - loss: 1.2154 - acc: 0.532 - ETA: 1:58 - loss: 1.2161 - acc: -  - ETA: 1:52 - loss: 1.2126 - acc: 0. - ETA: 1:52 - loss: 1.2132 - a - ETA: 1:51 - loss: 1.2114 - - ETA: 1:50 - loss: 1.2125 - ac - ETA: 1:49 - loss - ETA: 1:48  - ETA: 1:46 - loss: 1.2215 - acc: 0.53 - ETA: 1:45 - loss: 1.2 - ETA: 1:4 - ETA: 1:42 - loss: 1.2274   - ETA: 1:38 - loss: 1.2266 - acc: 0. - ETA: 1:37 - loss: 1.2 - ETA: 1:36 - lo - ETA: 1:34 - lo - ETA: 1:29 - loss: 1.2262 - acc: 0.528 - ETA: 1:29 - loss: 1.2259 - a - ETA: 1: - ETA: 1:18 - loss: 1.22 - ETA: 1:17 - loss - ETA: - ETA: 22s  - ETA: 19s -  - ETA: 15s - loss: 1.23 - ETA: 14s - lo - ETA: 7s - loss: 1.2315 - - ETA: 6s - loss: 1.2318 - acc: 0.529 - ETA: 6s - loss: 1.2317 - acc: 0 - ETA: 5s - loss: 1.2320 - acc - ETA: 5s - loss: 1.2321 - acc: 0.52 - ETA: 4s - loss: 1.2 - ETA: 0s - loss: 1.2319 - acc: 0.\n",
      "Epoch 36/50\n",
      "1767/1767 [==============================] - 149s 84ms/step - loss: 1.2373 - acc: 0.5214 - val_loss: 1.3379 - val_acc: 0.5358s - ETA: 2:00 - loss: 1.2574 - acc: 0.5 - ETA: 2:0 - ETA: 2:09 - loss: 1.2311 - acc - ETA: 2:09 - loss: 1.2243 - ac - ETA: 2:08 - loss: 1.2212 -  - ETA: 2:08 - loss: 1.2192 - acc: - ETA: 2:08 - loss: 1.2263  - ETA: 2:07 - loss: 1.2315 -  - ETA: 2:07 - loss: 1.2274 - acc: - - ETA: 1:44 - loss: 1.23 - ETA:  - ETA: 1 - ETA: 1:38 - loss - ETA: 1:36 - loss: 1.2337 - acc:  - ETA: 1:36 - lo - ETA: 1:34 - loss: 1.2338 - acc: 0.5 - ETA: 1:34 - loss: 1.2338 - acc: 0. - ETA: 1:33 - l  - E - ETA: 1:19 - loss: 1.2359 -  - ETA: 1:18 - l - ETA: 1:16 - loss: 1.2346 - acc: 0 - ETA: 1:15 - loss: 1.2350 -  - ETA: 27s - loss: 1.2368 - acc:  - ETA: 25s - loss: 1.23 -  - ETA: 8s - loss: 1.2 - ETA: 6s - loss: 1.2385 - acc - ETA: 5s - ETA: 3s - loss: 1.2385 - acc: 0 - ETA: 3s - loss: 1.2382 - - ETA: 2s -\n",
      "Epoch 37/50\n",
      "1767/1767 [==============================] - 148s 84ms/step - loss: 1.2363 - acc: 0.5223 - val_loss: 1.3705 - val_acc: 0.5208 2:04 -  - ETA: 2:04 - loss: 1.2210 -  - ETA: 2:04 - loss - ETA: 2:02 - loss: 1.2323 - acc: 0.521 - ETA: 2:02 - loss: 1.2311 - acc: 0.52 - ETA: 2:02 - loss: 1.2299 - acc: 0.5 - ETA: 2:02 - lo - ETA: 2:01 - loss: 1.22 - ETA: 2:00 - loss: 1.2241 - acc: 0.523 - ETA: 2:00 - loss: 1.2250 - - ETA: 1:56 - los - ETA: 1:54 - loss: 1.2296 - ac - ETA:  - ETA: 1:51 - loss: 1.2245 -  - ETA: 1:47 - loss: 1.2278 - acc: 0.52 - ETA: 1:47 - lo - ETA: 1:46 - loss: 1.2290 - acc: 0.522 - ETA: 1:45 - loss: 1.2284 -  - ETA: 1:45 - loss: 1.2298 -  - ETA: 1:44 - loss: 1.2303 - acc: 0.5 - ETA: 1:43 - loss: 1.2308 - acc:  - ETA: 1:43 - loss: 1.2304 - ETA: 1:42 - loss: 1.2306  - ETA - ETA: 1:35 - loss: 1.2368 - acc: 0.51 - ETA: 1:34 - loss: 1.2361 - acc: 0 - ETA: 1:34 - loss: 1.2360 - acc: 0 - ETA: 1:34 - loss: 1.23 - ETA: 1:26 - loss: 1. - ETA: 16s - lo - ETA: 10s - loss: 1.2344 - acc: 0. - ETA:  - ETA: 7s - loss: - - ETA: 3s - l - ETA: 1s - loss: 1.2365 \n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
      "Epoch 38/50\n",
      "1767/1767 [==============================] - 148s 84ms/step - loss: 1.2331 - acc: 0.5263 - val_loss: 1.3634 - val_acc: 0.5296A - ETA: 2:04 - loss: 1.24 - ETA: 2:0 - ETA: 2:02 - loss:  - ETA: 2:01 - loss:  - ETA: 2:00 - loss: 1.2311 - acc: 0.531 - ETA: 2:00 - loss: 1.2297 - acc: 0.532 - ETA: 2:00 - loss: 1.2296 - acc: 0 - ETA: 2:00 - loss: 1.2303 - acc: 0.532 - ETA: 1:59 - loss: 1.2320 - acc: 0.5 - ETA: 1:59 - loss: - E - ETA: 1:56 - loss: 1.2273 - ac - ETA: 1:55 - loss: 1.22 - ETA: 1:54 - loss:  - ETA: 1:50 - loss:  - ETA: 1:49 - loss: 1.2316 - a - ETA: 1:48 - loss: 1.2289 - acc: 0.528 - ETA: 1:48 - loss: 1.2284 - acc:  - ETA: 1:47 - loss: -  - ETA: 1:43 - loss - ETA: 1:41 - loss: 1.2339 - acc: 0. - ETA: 1:41 - loss: - ETA: 1:39 - loss: 1.23 - ETA: 1:38 -  - ETA: 1:36 - l - ETA: - ETA: 1:31 - loss: 1 - ETA: 1:30 - loss: 1.2351 - - ETA: - ETA: 19s - loss: 1.2318 - acc:  - ETA: 17s  - ETA: 16s - lo - ETA: 13s - loss: 1.2324 - acc - ETA: 11s - l - ETA: 8s - loss: 1.2321 - - E - ETA: 1s - loss: 1.2334 \n",
      "Epoch 39/50\n",
      "1767/1767 [==============================] - 148s 84ms/step - loss: 1.2407 - acc: 0.5189 - val_loss: 1.3584 - val_acc: 0.5298- ETA - ETA: 1:43 - loss: 1.2425 - acc: 0.521 - ETA: 1:43 - loss: 1.2429  - ETA: 1:42 - loss: 1. - ET - ETA:  - ETA: 0s - loss: 1.2410 - \n",
      "Epoch 40/50\n",
      "1767/1767 [==============================] - 149s 84ms/step - loss: 1.2247 - acc: 0.5309 - val_loss: 1.3738 - val_acc: 0.517104 - loss: 1.194 - ETA: 2:06 - loss: 1.1998 - acc:  - ETA: 2:06 - loss: 1.1960 - - ETA: 2:06 - loss: 1.2052 - acc: 0.532 - ETA: 2:06 - loss: 1.2051 - acc: 0.531 - ETA: 2:06 - loss: 1.2058 - a - ETA: 2:05 -  - ETA: 2:04 - loss: 1.2136 - ETA: 2:03 - loss: 1.2103 - a - ETA: 2:03 - loss:  - E - ETA: 1:59 - loss: 1. - ET - ETA: 1:55 - loss - ETA: 1:54 - loss:  - ETA: 1:46 - loss: - ETA: 1:44 - loss: 1.2196 - acc: 0 - ETA: 1:44 - loss:  - ETA: 1:29 - loss:  - ETA: 1:28 - loss: 1. - ETA: 1:26 - loss: 1. - ETA: 1:25 - loss: 1.2239 - acc: 0. - ETA: 1:25 - loss: 1.2236 - acc: 0.532 - ETA: 1:25 - loss: 1.2237 - a - ETA: 1:24 - loss: 1.2221 - acc - ETA - ETA: 1:17 - loss: 1.2211 -  - ETA: 1:1 - ETA: 1:04  - ETA: 1:02 - l - ETA: 59s - loss: 1.228 - E - ETA: 57s - loss: 1.2296 - acc:  - ETA: 57s - loss: 1.2298 - acc:  - ETA: 57s - loss: 1.2296 - acc: 0. - ETA: 57s - loss:  - ETA: 56s - loss: 1.2294 - acc:  - ETA: 56s -  - ETA: 50s - loss: 1. - ETA:  - ETA: 10s - loss: 1.2263 - a - ETA: 10s - loss: 1.2263 - acc - ETA:  - ETA: 7s - loss: 1.2263 -  - ETA: - ETA: 3s - loss: 1.2257 - a - ETA\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1767/1767 [==============================] - 148s 84ms/step - loss: 1.2372 - acc: 0.5220 - val_loss: 1.3313 - val_acc: 0.5358 - ETA: 2:02 - loss: 1.2344 - acc: 0. - ETA: - ETA: 2:02 - loss: 1.2430 - ETA: - ETA: 1:59 - loss: 1.2378 - - ETA: 1:59 - loss - ETA: 1:57 - loss: 1.2446 - acc: 0 - ETA: 1:57 - loss: 1.2480 -  - ETA: 1:48 - loss: 1.2495 - acc: 0 - ETA: 1:48 - loss: 1 - ETA: 1:46 - loss: 1.2477 -  - ETA: 1:45 - loss: 1.2503 - ac - ETA: 1:45 - loss: 1.2496 - acc - ETA: 1:44 - loss: 1.2501 -  - ETA: 10s - loss: 1.2371 - ETA: 9s - loss:  - ETA: 7s - - ETA: 5s - loss: 1.2368 - ETA: 4s - loss: 1.2375 - ETA:  - ETA: 0s - loss: 1.2371 - acc\n",
      "Epoch 42/50\n",
      "1767/1767 [==============================] - 149s 84ms/step - loss: 1.2321 - acc: 0.5276 - val_loss: 1.3930 - val_acc: 0.5244s: 1.2106 - ac - ETA: 1:58 - loss: 1.2297 - acc - - ETA: 2:07 - loss: 1.2187  - ETA: 2:06 - loss: 1.2325 - acc: - ETA: 2:05 - loss: 1.2401 - ac - ETA: 2:05 - loss: 1 - ETA: 2:04 - loss: 1.2346 - ETA: 1:59 - loss: 1.2344 - acc - ETA: 1:58 - loss: 1.2341 - acc: 0.52 - ETA: 1:58 - loss: 1.2363 - - ETA: 1:57 - loss: 1.2394 - ac - ETA: 1:53 - loss: 1.2 - ETA: 1:33 - loss: 1.2388 - acc: 0.5 - ETA: 1:33 - loss: 1.2398 - acc: 0.52 - ETA: 1:33 - loss: 1.2396 - acc: 0.52 - ETA: 1:33 - loss: 1.24 - ETA: 1:31 - loss: 1.2420 - acc: 0. - ETA: 1:31 -  - ETA: 1:22 - loss: 1.2404 - acc: 0.5 - ETA: 1:22 - loss: 1 - - ETA: 1:14 - loss: 1.2351 - acc:  - ET - ETA: 9s - loss: 1.2322 - acc: 0. - ETA: 9s - loss:  - ETA: 4s - lo - ETA:\n",
      "Epoch 43/50\n",
      "1767/1767 [==============================] - 149s 84ms/step - loss: 1.2363 - acc: 0.5204 - val_loss: 1.3537 - val_acc: 0.5219 - loss:  - ETA: 2:05 - loss: 1.2302 -  - ETA: 2:05 - loss: 1.2236  - ETA: 2:06 - loss: 1.2305 - acc: 0. - ETA: 2:06 - - ETA - ETA: 1:58 - loss: 1.2236 - acc:  - ETA: 1:58 - loss: 1.2262 - ac - ETA: 1:57 - loss: 1.2240 - a - ETA: 1:56 - loss:   - ETA: 1:43 - loss: 1.236 - ETA: 1:41 - loss: 1.2357 - acc: - ETA: 1:41 - lo - ETA: 1:39 - loss: 1.2 - ETA: 1:38 - loss: 1.2360 - acc: 0 - ETA: 1:37 - - ETA: 1:35 - loss: 1.2390 - acc:  - ETA: 1:35 - loss: 1.2394 - ETA: 1:34 - loss: 1.2415 - acc: - ETA: 1:33 - loss: 1.2422 - acc: 0.514 - ETA: 1:33 -  - ETA: 1:31 - - ETA: 1:25 - loss: 1. - ETA: 1:24 - loss: 1.2376 - acc: 0. - ETA: 1:24 - loss: 1.2372 - acc: 0.51 - ETA: 1:23 - loss: 1.237 - ETA: 1:22 - lo - ETA: 5s - loss: 1.2368 -  - ETA: 1s - loss: 1.2362 - ETA: 0s - loss: 1.2361 - acc: 0.\n",
      "Epoch 44/50\n",
      "1767/1767 [==============================] - 149s 84ms/step - loss: 1.2344 - acc: 0.5260 - val_loss: 1.3367 - val_acc: 0.5327a - ETA: 2:07 - loss - ETA: 2:06 - loss: 1.2447 - acc:  - ETA: 2:06 - los - ETA: 2:05 - loss: 1.2344  - ETA: 2:04 - loss: 1.2336 - acc: 0 - ETA: 2:04 - loss: 1.2402 - a - ETA: 1:58 - loss: 1.2437 - acc: - ETA: 1:58 - loss: 1.2455 -  - ETA: 1:57 - loss: 1.24 - ETA: 1:56 - loss: 1.2445 - ac - ETA: 1:55 - loss: 1.2456 - a - ETA: 1:55 - loss: 1.2454 - ac - ETA: 1:54 - loss:  - ETA: 1:53 - loss: 1.241 - ETA: 1:52 - loss:  - ETA: 1:50 - ETA:  - - ETA: 1:42 - loss: 1.2382 - acc: 0 - ETA: 1:36 - loss: 1.2393 - ac - ETA: 1:35 - loss: 1.2380 - acc: 0.522 - ETA: 1:35 - loss - ETA: 1:33 - los - ETA: 1:31 - loss: 1. - ETA: 1:30 - loss: 1.2358 - acc: 0.523 - ETA: 1:30 - loss: 1.2355 - acc:  - ETA: 1:29 - loss: 1.2357  - ETA: 1 - ETA:  - ETA: 1:23 - loss: 1.2341 -  - ETA: 1:22 - loss: 1.2344 - acc: 0.526 - ETA: 1:22 - loss: 1.2343 - acc: 0.526 - ETA: 1:22 -  - E - ETA: 8s - loss:  - ETA: 3s -  - ETA: 1s - loss: 1.235\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
      "Epoch 45/50\n",
      "1767/1767 [==============================] - 149s 84ms/step - loss: 1.2346 - acc: 0.5249 - val_loss: 1.3637 - val_acc: 0.5256 -  - ETA: 2:06 - loss: 1.2333 - acc: - ETA: 2:06 - loss: 1.2302 - acc: 0.51 - ETA: 2:06 -  - ETA: 2:03 - loss: 1.2456 -  - ETA: 2:02 - loss: 1.24 - ETA: 1:56 - loss: 1.2474 - acc: 0 - ETA: 1:56 - loss: 1.2469 - acc:  - ETA: 1:56 - loss: 1.2463 - acc: 0. - ETA: 1:55 - loss: 1.2449 - acc:  - - ETA: 1:52 - loss: 1.2482 - acc: 0.51 - ETA: 1:52 - loss: 1.2493 - acc - ETA: 1:51 - loss: 1.2495 - acc:  - ETA: 1:51 - loss: 1.2508 - ETA: 1:50 - loss: 1.2526 - a - ETA: 1 - ETA: 1:37 - loss: 1.2333 - acc: 0.52 - ETA: 1:37 - loss: 1.2328 - acc: 0 - ETA: 1:36 - loss: 1.2327 - a - ETA: 1:36 - loss: - ETA: 1:34 - loss: 1.2308 - acc - ETA: 1:33 - loss: - ETA: 1:32 - loss: 1. - ETA: 1:30 - los - ETA: 1:25 - l - ETA: 1:23 - loss:  - ETA: 1:21 - loss: 1. - ETA: 1:16 - loss: 1 - ETA: 1:15 - loss: 1.2302 - acc:  - ETA: 1:14 - loss: 1.2299 - acc - ETA: 1:14 - loss: 1.2300 - acc: 0 - ET - ETA: 5s - loss: 1.2351 - acc: - ETA: 4s - loss: 1.2354 - ETA: 3s - loss: 1.2352 - acc: - ETA: 3s - loss: 1.2353 - acc - ETA: 2s - loss: 1.2356 - acc - ETA: 1s - loss: 1.2354 - acc: 0.525 - ETA: 1s - loss: \n",
      "Epoch 46/50\n",
      "1767/1767 [==============================] - 148s 84ms/step - loss: 1.2312 - acc: 0.5235 - val_loss: 1.3575 - val_acc: 0.5296 2:04 - loss: 1.2660 - acc:  - ETA: 2:04 -  - ETA: 2:05 - loss: 1.2662  - ETA: 2:05 - loss: 1.2697 - acc: 0.5 - ETA - ETA: 1:59 - loss: 1.2676  - ETA: 1:59 - loss: 1.2690 - acc: - ETA: 1:58 - loss: 1.2695 - ac - ETA: 1:58 - loss: 1.2679 - ETA: - ETA: 1s - loss\n",
      "Epoch 47/50\n",
      "1767/1767 [==============================] - 148s 84ms/step - loss: 1.2281 - acc: 0.5272 - val_loss: 1.3806 - val_acc: 0.526703 - loss: 1.2508 - ETA: 2 - ETA: 1:56 - loss: 1.2375  - ETA: 1:55 - loss:  - ETA: 1:47 - loss - ETA: 1:46 - loss: 1.2320 - acc: 0. - ETA: 1:46  - ETA: 1:37 - loss: 1.2359 - acc - ETA: 1: - ETA: 1:18 - loss: 1.2283 - ac - ETA: 1:17 - loss: 1. - ETA: 1:16 -  - ETA: 1:14 - loss:  - ETA: 1:12 - l - ETA: 1:10 - loss: 1.2327 - ac - ETA: 1:09 - loss: 1.2332 -   - ETA: 1:05 - loss: 1.2340 - acc: 0 - ETA: 1:05 - l - ETA: 1:03 - los - ETA: 1:01  - ETA:  - ETA: 17 - ETA - ETA: 14s - loss: 1. - ETA: 1s - loss: 1\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
      "Epoch 48/50\n",
      "1767/1767 [==============================] - 149s 85ms/step - loss: 1.2308 - acc: 0.5282 - val_loss: 1.3674 - val_acc: 0.5279 loss: - ETA: 2:05 - loss: 1.2242  - ETA: 2:06 - loss: 1.2152 - acc: 0. - ETA: 2:06 - loss: 1.2091 - acc: 0. - ETA: - ETA - ETA: 2: - ETA: 1:5 - ETA: 1:56 - loss: 1.2207 - acc: 0 - ETA: 1:49 - loss: 1.2200 - acc: 0.534 - ETA: 1:49 - loss: 1.2201 - acc: 0. - ETA: 1:49 -  - ETA: 1:47 - loss: 1.2234 - acc: 0.53 - ETA: 1:46 - loss: 1.2230 - - ETA: 1:45 - loss - ETA: 1:44 - loss: 1.2256 - - ETA: 1:43 - loss: 1.2253 - ac - ETA: 1:42 - loss: 1.2232 - ac - - ETA: 1:39 - loss: 1.2291 - acc: 0.530 - ETA: 1 - ETA: 1:36 - loss: 1.2280 - - ETA: 1:35 - loss: 1.2287 - acc: 0.529 - ETA: 1:35 - loss: 1.228 - ETA: 1:27 - loss: 1.2285  - ETA: 1:26 - loss: 1. - ETA:  - ETA: 1: - ETA: 1:20 - loss: 1.2322 - - ETA: 1:19 - loss: 1.2341 - ETA: 1:18 - los - ETA: 1:16 - loss: 1.2319 - acc: 0.526 - ETA: - ETA: 1:13 - loss: 1.2298 - acc: - ETA: 1:12 - loss: 1.2305 - acc: 0 - ETA: 1:12 - loss: 1.2306 - - ETA: - ETA: 1:05 - loss: 1.2331 - acc: 0 - ETA: 1:05 - los - ETA: 58s - loss:  - ETA: 47s - loss: 1.2353 - a - - ETA: 45s  - ETA: 41s - lo - ETA - ETA: 38s - loss: 1.2395 - acc: 0.52 - ETA: 38s - loss - E - ETA: 36s - loss - ETA: 35s - loss: 1.2382 - acc: 0. - ETA: 35s - loss: 1.2380 - acc: 0.52 - ETA: 35s -  - - ETA: 32s - loss: 1.2378 - acc: 0. - ETA: 32s - loss:  - ETA: 23s -  - ETA: 22s - loss: 1.2356 - a - E - ETA: 4s - loss: - ETA: 2s - loss: 1.2310 - acc: 0. - ETA: 2s - loss: 1.231 - ETA: 1s - loss: 1.2313 - acc: 0.5 - ETA: 1s - loss: 1.2312 \n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1767/1767 [==============================] - 148s 84ms/step - loss: 1.2321 - acc: 0.5262 - val_loss: 1.3453 - val_acc: 0.5307ETA: 1:41 - loss: 1.2308 - acc: - ETA: 1:41 - loss: 1.2307  - ETA: 1:40 - loss: 1. - ETA: 7s - loss: 1.230\n",
      "Epoch 50/50\n",
      "1767/1767 [==============================] - 149s 84ms/step - loss: 1.2357 - acc: 0.5248 - val_loss: 1.3796 - val_acc: 0.5259- ETA: 1:58 - ETA: 2:02 - loss: 1.2476 - ac - ETA: 2:03 -  - ETA: 2:04 - ETA: 2:03 - - ETA - ETA: 2:00 - ETA: 1:58 - loss: 1.2370 - acc: 0.52 - ETA: 1:57 - loss: 1.237 - ETA: 1:56 - loss: 1 - ETA: 1:55 - - ETA: 1:51 - loss: 1.2404 - acc - ETA: 1:50 - loss: 1. - ETA: 1: - ETA: 1:46 - loss: 1.2409 - ETA: 1:45 - loss: 1 - ETA: 1:31 - loss: 1.237 - ETA: 1:30 - loss: 1.2365 - acc: 0 - ETA: 1:29 - loss: 1.2361 - acc: 0.522 - ETA: 1:29 - loss: 1.23 - ETA: 1:28 - loss: 1.2360 - acc: 0.52 - ETA: 1:28 - loss: 1.2368 - a - ETA: 1:2 - ETA: 1:25 - loss: 1.2383  - ETA: 1:24 - loss: 1.2411 -  - ETA: 1:23 - loss: 1.2416 - acc: 0.52 - ETA: 1:23 - loss: 1.2411 - acc: 0 - ETA: 1:22 - loss: 1. - ETA: 1:11 - loss: 1.2403 - acc: - ETA: 1:10 - l - ETA - E - ETA: 51s  - - ETA: 12s  - ETA: 11s -  - ETA: 10s - loss:  - ETA: 6s - loss: 1.2 - ETA: 4s - l - ETA:\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.0240001069306004e-10.\n"
     ]
    }
   ],
   "source": [
    "checkpoint=ModelCheckpoint(r'C:\\Users\\Mohit\\Documents\\Interested ML Projects\\Emotion Detector DL\\emotion_little_vgg_3.h5')\n",
    "'''earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 3,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)'''\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 3, verbose = 1, min_delta = 0.0001)\n",
    "\n",
    "# we put our call backs into a callback list\n",
    "callbacks = [ checkpoint, reduce_lr]#earlystop,\n",
    "\n",
    "# We use a very small learning rate \n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = Adam(lr=0.001),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "nb_train_samples = 28273\n",
    "nb_validation_samples = 3534\n",
    "epochs = 50\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = nb_train_samples // batch_size,\n",
    "    epochs = epochs,\n",
    "    callbacks = callbacks,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing that with Video Interface using OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "classifier = load_model('/home/deeplearningcv/DeepLearningCV/Trained Models/emotion_little_vgg_3.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
